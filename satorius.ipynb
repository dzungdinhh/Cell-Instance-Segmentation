{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### 1. Imports ","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport time\nimport collections\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.transforms import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor","metadata":{"execution":{"iopub.status.busy":"2021-11-22T06:21:13.250784Z","iopub.execute_input":"2021-11-22T06:21:13.251089Z","iopub.status.idle":"2021-11-22T06:21:15.755287Z","shell.execute_reply.started":"2021-11-22T06:21:13.251009Z","shell.execute_reply":"2021-11-22T06:21:15.754536Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def random_seed(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n    \nrandom_seed(1702)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T06:21:15.757202Z","iopub.execute_input":"2021-11-22T06:21:15.757468Z","iopub.status.idle":"2021-11-22T06:21:15.812278Z","shell.execute_reply.started":"2021-11-22T06:21:15.757433Z","shell.execute_reply":"2021-11-22T06:21:15.811624Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### 2. Configuration","metadata":{}},{"cell_type":"code","source":"width = 704\nheight = 520\ncell_dict = {\"astro\": 1, \"cort\": 2, \"shsy5y\": 3}\nmask_threshold_dict = {1: 0.55, 2: 0.75, 3:  0.6}\nmin_score_dict = {1: 0.55, 2: 0.75, 3: 0.5}\nnum_box_detections = 600\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nDEVICE = device\nn_epochs = 3\nbatch_size = 2\nlearning_rate = 0.001\nmomentum = 0.9\nweight_decay = 0.0005 \n\nNORMALIZE = True\nRESNET_MEAN = (0.485, 0.456, 0.406)\nRESNET_STD = (0.229, 0.224, 0.225)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T06:39:24.464098Z","iopub.execute_input":"2021-11-22T06:39:24.464808Z","iopub.status.idle":"2021-11-22T06:39:24.471147Z","shell.execute_reply.started":"2021-11-22T06:39:24.464770Z","shell.execute_reply":"2021-11-22T06:39:24.470208Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### 3. Helpers ","metadata":{}},{"cell_type":"code","source":"# ref: https://www.kaggle.com/inversion/run-length-decoding-quick-start\ndef rle_decode(mask_rle, shape, color=1):\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height, width, channels) of array to return\n    color: color for the mask\n    Returns numpy array (mask)\n\n    '''\n    s = mask_rle.split()\n\n    starts = list(map(lambda x: int(x) - 1, s[0::2]))\n    lengths = list(map(int, s[1::2]))\n    ends = [x + y for x, y in zip(starts, lengths)]\n    if len(shape)==3:\n        img = np.zeros((shape[0] * shape[1], shape[2]), dtype=np.float32)\n    else:\n        img = np.zeros(shape[0] * shape[1], dtype=np.float32)\n    for start, end in zip(starts, ends):\n        img[start : end] = color\n\n    return img.reshape(shape)\n\n\ndef rle_encoding(x):\n    dots = np.where(x.flatten() == 1)[0]\n    run_lengths = []\n    prev = -2\n    for b in dots:\n        if (b>prev+1): run_lengths.extend((b + 1, 0))\n        run_lengths[-1] += 1\n        prev = b\n    return ' '.join(map(str, run_lengths))\n\n\ndef remove_overlapping_pixels(mask, other_masks):\n    for other_mask in other_masks:\n        if np.sum(np.logical_and(mask, other_mask)) > 0:\n            mask[np.logical_and(mask, other_mask)] = 0\n    return mask\n\ndef combine_masks(masks, mask_threshold):\n    \"\"\"\n    combine masks into one image\n    \"\"\"\n    maskimg = np.zeros((height, width))\n    # print(len(masks.shape), masks.shape)\n    for m, mask in enumerate(masks,1):\n        maskimg[mask>mask_threshold] = m\n    return maskimg\n\n\ndef get_filtered_masks(pred):\n    \"\"\"\n    filter masks using MIN_SCORE for mask and MAX_THRESHOLD for pixels\n    \"\"\"\n    use_masks = []   \n    for i, mask in enumerate(pred[\"masks\"]):\n\n        # Filter-out low-scoring results. Not tried yet.\n        scr = pred[\"scores\"][i].cpu().item()\n        label = pred[\"labels\"][i].cpu().item()\n        if scr > min_score_dict[label]:\n            mask = mask.cpu().numpy().squeeze()\n            # Keep only highly likely pixels\n            binary_mask = mask > mask_threshold_dict[label]\n            binary_mask = remove_overlapping_pixels(binary_mask, use_masks)\n            use_masks.append(binary_mask)\n\n    return use_masks","metadata":{"execution":{"iopub.status.busy":"2021-11-22T06:21:15.822358Z","iopub.execute_input":"2021-11-22T06:21:15.822701Z","iopub.status.idle":"2021-11-22T06:21:15.840296Z","shell.execute_reply.started":"2021-11-22T06:21:15.822666Z","shell.execute_reply":"2021-11-22T06:21:15.839540Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Ref: https://www.kaggle.com/theoviel/competition-metric-map-iou\n        \ndef compute_iou(labels, y_pred, verbose=0):\n    \"\"\"\n    Computes the IoU for instance labels and predictions.\n\n    Args:\n        labels (np array): Labels.\n        y_pred (np array): predictions\n\n    Returns:\n        np array: IoU matrix, of size true_objects x pred_objects.\n    \"\"\"\n\n    true_objects = len(np.unique(labels))\n    pred_objects = len(np.unique(y_pred))\n\n    if verbose:\n        print(\"Number of true objects: {}\".format(true_objects))\n        print(\"Number of predicted objects: {}\".format(pred_objects))\n\n    # Compute intersection between all objects\n    intersection = np.histogram2d(\n        labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects)\n    )[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins=true_objects)[0]\n    area_pred = np.histogram(y_pred, bins=pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n    intersection = intersection[1:, 1:] # exclude background\n    union = union[1:, 1:]\n    union[union == 0] = 1e-9\n    iou = intersection / union\n    \n    return iou  \n\ndef precision_at(threshold, iou):\n    \"\"\"\n    Computes the precision at a given threshold.\n\n    Args:\n        threshold (float): Threshold.\n        iou (np array): IoU matrix.\n\n    Returns:\n        int: Number of true positives,\n        int: Number of false positives,\n        int: Number of false negatives.\n    \"\"\"\n    matches = iou > threshold\n    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n    false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n    tp, fp, fn = (\n        np.sum(true_positives),\n        np.sum(false_positives),\n        np.sum(false_negatives),\n    )\n    return tp, fp, fn\n\ndef iou_map(truths, preds, verbose=0):\n    \"\"\"\n    Computes the metric for the competition.\n    Masks contain the segmented pixels where each object has one value associated,\n    and 0 is the background.\n\n    Args:\n        truths (list of masks): Ground truths.\n        preds (list of masks): Predictions.\n        verbose (int, optional): Whether to print infos. Defaults to 0.\n\n    Returns:\n        float: mAP.\n    \"\"\"\n    ious = [compute_iou(truth, pred, verbose) for truth, pred in zip(truths, preds)]\n\n    if verbose:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        tps, fps, fns = 0, 0, 0\n        for iou in ious:\n            tp, fp, fn = precision_at(t, iou)\n            tps += tp\n            fps += fp\n            fns += fn\n\n        p = tps / (tps + fps + fns)\n        prec.append(p)\n\n        if verbose:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tps, fps, fns, p))\n\n    if verbose:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n\n    return np.mean(prec)\n\n\ndef get_score(ds, mdl):\n    \"\"\"\n    Get average IOU mAP score for a dataset\n    \"\"\"\n    mdl.eval()\n    iouscore = 0\n    for i in tqdm(range(len(ds))):\n        img, targets = ds[i]\n        with torch.no_grad():\n            result = mdl([img.to(DEVICE)])[0]\n            \n        masks = combine_masks(targets['masks'], 0.5)\n        labels = pd.Series(result['labels'].cpu().numpy()).value_counts()\n\n        mask_threshold = mask_threshold_dict[labels.sort_values().index[-1]]\n        pred_masks = combine_masks(get_filtered_masks(result), mask_threshold)\n        iouscore += iou_map([masks],[pred_masks])\n    return iouscore / len(ds)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T06:21:15.844417Z","iopub.execute_input":"2021-11-22T06:21:15.844601Z","iopub.status.idle":"2021-11-22T06:21:15.865938Z","shell.execute_reply.started":"2021-11-22T06:21:15.844579Z","shell.execute_reply":"2021-11-22T06:21:15.864504Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Ref https://www.kaggle.com/abhishek/maskrcnn-utils\n\nclass Compose:\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, image, target):\n        for t in self.transforms:\n            image, target = t(image, target)\n        return image, target\n\nclass VerticalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-2)\n            bbox = target[\"boxes\"]\n            bbox[:, [1, 3]] = height - bbox[:, [3, 1]]\n            target[\"boxes\"] = bbox\n            target[\"masks\"] = target[\"masks\"].flip(-2)\n        return image, target\n\nclass HorizontalFlip:\n    def __init__(self, prob):\n        self.prob = prob\n\n    def __call__(self, image, target):\n        if random.random() < self.prob:\n            height, width = image.shape[-2:]\n            image = image.flip(-1)\n            bbox = target[\"boxes\"]\n            bbox[:, [0, 2]] = width - bbox[:, [2, 0]]\n            target[\"boxes\"] = bbox\n            target[\"masks\"] = target[\"masks\"].flip(-1)\n        return image, target\n\nclass Normalize:\n    def __call__(self, image, target):\n        image = F.normalize(image, RESNET_MEAN, RESNET_STD)\n        return image, target\n\nclass ToTensor:\n    def __call__(self, image, target):\n        image = F.to_tensor(image)\n        return image, target\n    \n\ndef get_transform(train):\n    transforms = [ToTensor()]\n    if NORMALIZE:\n        transforms.append(Normalize())\n    \n    # Data augmentation for train\n    if train: \n        transforms.append(HorizontalFlip(0.5))\n        transforms.append(VerticalFlip(0.5))\n\n    return Compose(transforms)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T06:21:15.867588Z","iopub.execute_input":"2021-11-22T06:21:15.868037Z","iopub.status.idle":"2021-11-22T06:21:15.882908Z","shell.execute_reply.started":"2021-11-22T06:21:15.868000Z","shell.execute_reply":"2021-11-22T06:21:15.882058Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"### 4. Dataloader","metadata":{}},{"cell_type":"code","source":"class SatoriusDataset(Dataset): \n    def __init__(self, csv_file, image_dir, transform=None, resize=None):\n        self.transform = transform\n        self.csv = csv_file \n        self.resize = True if resize else False \n        \n        # resize is the factor, e.g 1.2, 1.3, ... \n        if self.resize: \n            self.height = int(height * resize)\n            self.width = int(width * resize)\n        else: \n            self.height = height\n            self.width = width\n        \n        # collections.defaultdict returns an empty dict when the key is not found\n        self.image_info = collections.defaultdict(dict)\n        \n        # group by id then cell, and get the annotations \n        csv_groupby = self.csv.groupby(['id', 'cell_type'])['annotation'].agg(lambda x: list(x)).reset_index()\n        \n        \n        # image_info is the replacement of csv \n        # use image_info to iterate\n        for index, row in csv_groupby.iterrows(): \n            self.image_info[index] = { \n                'image_id': row['id'],\n                'image_path': os.path.join(image_dir, row['id'] + '.png'),\n                'annotations': row['annotation'],\n                'cell_type': cell_dict[row['cell_type']],\n            }        \n        \n    def __len__(self): \n        return len(self.image_info)\n\n    def get_boxes(self, mask):\n        # the size of the mask is [w, h] and the values are True and False\n        # using np.where will return the location of the True value in the mask \n        mask = np.where(mask)\n\n        # locations of the bounding box \n        x_min = np.min(mask[1])\n        x_max = np.max(mask[1])\n        y_min = np.min(mask[0])\n        y_max = np.max(mask[0])\n\n        return [x_min, y_min, x_max, y_max]\n\n    def __getitem__(self, idx): \n        item = self.image_info[idx]\n\n        image = cv2.imread(item['image_path'], cv2.IMREAD_COLOR)        \n        if self.resize: \n            image = cv2.resize(image, (self.width, self.height))\n        \n        # height goes before width because of rle_decode()\n        masks = np.zeros((len(item['annotations']), self.height, self.width), dtype=np.uint8)\n        boxes = []\n\n        for i, mask in enumerate(item['annotations']): \n            mask = rle_decode(mask, (height, width))\n\n            if self.resize: \n                mask = cv2.resize(mask, (self.width, self.height))\n                \n            # the mask only contains true and false, which are 1 and 0\n            mask = np.array(mask) > 0\n            masks[i,:,:] = mask\n            boxes.append(self.get_boxes(mask))\n\n        labels = [item['cell_type'] for _ in range(len(item['annotations']))]\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        masks = torch.as_tensor(masks, dtype=torch.uint8)\n        \n        # targets must have for the mask-rcnn\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"masks\": masks,\n        }\n\n        if self.transform: \n            image, target = self.transform(image, target)\n\n        return image, target","metadata":{"execution":{"iopub.status.busy":"2021-11-22T06:21:15.884564Z","iopub.execute_input":"2021-11-22T06:21:15.884913Z","iopub.status.idle":"2021-11-22T06:21:15.904402Z","shell.execute_reply.started":"2021-11-22T06:21:15.884875Z","shell.execute_reply":"2021-11-22T06:21:15.903379Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# test the dataset\n\ntest_dataset_csv = pd.read_csv(\"../input/sartorius-cell-instance-segmentation/train.csv\")\ntest_dataset = SatoriusDataset(test_dataset_csv, \"../input/sartorius-cell-instance-segmentation/train\")\n\ntest = test_dataset.__getitem__(1)\nimg = test[0]\ntarget = test[1]\n\nboxes = target['boxes']\nmasks = target['masks']\nlabels = target['labels']\n\nprint(f\"img  :  {img.shape}\")\nprint(f\"boxes:  {boxes.shape}\")\nprint(f\"labels: {labels.shape}\")\nprint(f\"masks:  {masks.shape}\")\n\nprint(\"labels:   \", labels.unique())\n\n_, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\nax[0].imshow(img)\nax[1].imshow(combine_masks(masks, 0))\n\nfor box in boxes:\n    # box[1] - box[3] because y-axis is reversed\n    ax[1].add_patch(patches.Rectangle((box[0], box[3]), box[2] - box[0], box[1] - box[3], linewidth=1, edgecolor='r', facecolor='none'))\n    \nplt.axis(False)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T06:21:15.905661Z","iopub.execute_input":"2021-11-22T06:21:15.906054Z","iopub.status.idle":"2021-11-22T06:21:17.473392Z","shell.execute_reply.started":"2021-11-22T06:21:15.906017Z","shell.execute_reply":"2021-11-22T06:21:17.472525Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### 5. Model ","metadata":{}},{"cell_type":"code","source":"class MaskRCNN(nn.Module):\n    def __init__(self, num_classes, pretrained_model=None):\n        super().__init__()\n        \n        self.pretrained_model = pretrained_model\n        \n        if NORMALIZE: \n            self.model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n                                                                      box_detections_per_img=num_box_detections, \n                                                                      image_mean=RESNET_MEAN, \n                                                                      image_std=RESNET_STD)\n        else:\n            self.model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True,\n                                                                      box_detections_per_img=num_box_detections)\n\n        hidden_layer = 256\n        \n        # get the input features of the box_predictor \n        # replace the pretrained box_predictor layer with new FastRCNN\n        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes+1)\n        \n        # get the input features of the mask_predictor \n        # replace the pretrained mask_predictor layer with new MaskRCNN\n        in_features_mask = self.model.roi_heads.mask_predictor.conv5_mask.in_channels\n        self.model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes+1)\n                \n    def forward(self, x, y=None): \n        # in the training mode, the model needs both x and y (y is the target)\n        # in the val mode, the model only needs x\n        if y: \n            return self.model(x, y)\n        \n        return self.model(x)\n    \n#     def init_model(self):\n#         if self.pretrained_model:\n#             self.model.load_state_dict(self.pretrained_model, map_location=device)\n        \n#         print(\"Loaded pretrained weights\")","metadata":{"execution":{"iopub.status.busy":"2021-11-22T07:30:13.660607Z","iopub.execute_input":"2021-11-22T07:30:13.661509Z","iopub.status.idle":"2021-11-22T07:30:13.670638Z","shell.execute_reply.started":"2021-11-22T07:30:13.661453Z","shell.execute_reply":"2021-11-22T07:30:13.669755Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"markdown","source":"### 6. Training","metadata":{}},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, optimizer, device, scheduler):\n        self.model = model\n        self.optimizer = optimizer\n        self.scheduler = scheduler\n        self.device = device\n        \n        self.train_loss_mask = []\n        self.train_loss_classifier = []\n        self.train_loss = []\n        self.val_loss_mask = []\n        self.val_loss_classifier = []\n        self.val_loss = []\n        \n    def fit(self, train_loader, val_loader, n_epochs):\n        for epoch in range(n_epochs): \n            \n            # train and val\n            self.train_epoch(train_loader)            \n            self.val_epoch(val_loader)\n            \n            if self.scheduler: \n                self.scheduler.step() \n            \n            print(f\"epoch {epoch + 1} train:  mask loss : {self.train_loss_mask[-1]:3f}, classifier loss:  {self.train_loss_classifier[-1]:3f}\")\n            print(f\"epoch {epoch + 1} val:    mask loss : {self.val_loss_mask[-1]:3f},   classifier loss: {self.val_loss_classifier[-1]:3f}\")\n            print(f\"epoch {epoch + 1}:        train loss: {self.train_loss[-1]:3f},      val_loss       : {self.val_loss[-1]:3f}\")\n\n            print('save model ...')\n            torch.save(self.model.state_dict(), f'model-e{epoch + 1}.pt')\n\n            print('\\n\\n')\n                                \n    def train_epoch(self, train_loader):\n        self.model.train()\n        self.train_loss_mask.append(0)\n        self.train_loss_classifier.append(0)\n        self.train_loss.append(0)\n\n        for i, data in tqdm(enumerate(train_loader, 0)): \n            x, y = data\n            x = list(img.to(self.device) for img in x)\n            y = [{k: v.to(self.device) for k, v in t.items()} for t in y]\n            \n            self.optimizer.zero_grad()\n            \n            # in training mode, the model return the loss\n            # in val mode, the model return the predictions\n            losses = self.model(x, y)\n            loss = sum(loss for loss in losses.values())\n            loss.backward()\n            self.optimizer.step()\n            \n            self.train_loss_mask[-1] += losses['loss_mask'].item()\n            self.train_loss_classifier[-1] += losses['loss_classifier'].item()\n            self.train_loss[-1] += loss.item()\n        \n        # gets the average\n        self.train_loss_mask[-1] /= len(train_loader)\n        self.train_loss_classifier[-1] /= len(train_loader)\n        self.train_loss[-1] /= len(train_loader)\n        \n    def val_epoch(self, val_loader):\n        self.val_loss_mask.append(0)\n        self.val_loss_classifier.append(0)\n        self.val_loss.append(0)\n\n        for i, data in tqdm(enumerate(val_loader, 0)): \n            with torch.no_grad():\n                x, y = data \n                x = list(img.to(self.device) for img in x)\n                y = [{k: v.to(self.device) for k, v in t.items()} for t in y]\n                \n                # in training mode, the model return the loss\n                # in val mode, the model return the predictions\n                losses = model(x, y)\n                loss = sum(loss for loss in losses.values())\n                \n                self.val_loss_mask[-1] += losses['loss_mask'].item()\n                self.val_loss_classifier[-1] += losses['loss_classifier'].item()\n                self.val_loss[-1] += loss.item()\n\n        # gets the average\n        self.val_loss_mask[-1] /= len(val_loader)\n        self.val_loss_classifier[-1] /= len(val_loader)\n        self.val_loss[-1] /= len(val_loader)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T06:21:17.489072Z","iopub.execute_input":"2021-11-22T06:21:17.489652Z","iopub.status.idle":"2021-11-22T06:21:17.524850Z","shell.execute_reply.started":"2021-11-22T06:21:17.489612Z","shell.execute_reply":"2021-11-22T06:21:17.524040Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"transform = get_transform(False)\n\ntrain_csv = pd.read_csv(\"../input/sartorius-cell-instance-segmentation/train.csv\")\n\n# the layout of the df_images would be: id, cell_type, annotation\n# there would be one row for one image because each image only contains one cell_type\ndf_images = train_csv.groupby([\"id\", \"cell_type\"]).agg({'annotation': 'count'}).sort_values(\"annotation\", ascending=False).reset_index()\n\ntrain_df, test_df = train_test_split(df_images, stratify=df_images['cell_type'], \n                                                  test_size=0.2,\n                                                  random_state=1702)\n\n# dataloader for training\ntrain_df = train_csv[train_csv['id'].isin(train_df['id'])]\ntrain_dataset = SatoriusDataset(train_df, \"../input/sartorius-cell-instance-segmentation/train\", transform=transform)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True,\n                              num_workers=2, collate_fn=lambda x: tuple(zip(*x)))\n\n# dataloader for validation\ntest_df = train_csv[train_csv['id'].isin(test_df['id'])]\ntest_dataset = SatoriusDataset(test_df, \"../input/sartorius-cell-instance-segmentation/train\", transform=transform)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, pin_memory=True,\n                             num_workers=2, collate_fn=lambda x: tuple(zip(*x)))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T06:21:17.526190Z","iopub.execute_input":"2021-11-22T06:21:17.526521Z","iopub.status.idle":"2021-11-22T06:21:17.988991Z","shell.execute_reply.started":"2021-11-22T06:21:17.526487Z","shell.execute_reply":"2021-11-22T06:21:17.988278Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"model = MaskRCNN(len(cell_dict))\nmodel.to(device)\n\nfor param in model.parameters():\n    param.requires_grad = True\n    \noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\ntrainer = Trainer(model, optimizer, device, lr_scheduler)\ntrainer.fit(train_dataloader, test_dataloader, n_epochs)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T06:21:17.991180Z","iopub.execute_input":"2021-11-22T06:21:17.991567Z","iopub.status.idle":"2021-11-22T06:32:09.060329Z","shell.execute_reply.started":"2021-11-22T06:21:17.991531Z","shell.execute_reply":"2021-11-22T06:32:09.059332Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_loss_mask = trainer.train_loss_mask\ntrain_loss_classifier = trainer.train_loss_classifier\ntrain_loss = trainer.train_loss\nval_loss_mask = trainer.val_loss_mask\nval_loss_classifier = trainer.val_loss_classifier\nval_loss = trainer.val_loss\n\nfig, ax = plt.subplots(1, 3, figsize=(17, 5))\nax[0].plot(train_loss_mask, color='red', label=\"train\")\nax[0].plot(val_loss_mask, color='blue', label=\"val\")\nax[1].plot(train_loss_classifier, color='red', label=\"train\")\nax[1].plot(val_loss_classifier, color='blue', label=\"val\")\nax[2].plot(train_loss, color='red', label=\"train\")\nax[2].plot(val_loss, color='blue', label=\"val\")\n\nax[0].title.set_text(\"mask_loss\")\nax[1].title.set_text(\"classifier_loss\")\nax[2].title.set_text(\"total_loss\")\n\nax[0].legend()\nax[1].legend()\nax[2].legend()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T07:15:01.390062Z","iopub.execute_input":"2021-11-22T07:15:01.390836Z","iopub.status.idle":"2021-11-22T07:15:01.923817Z","shell.execute_reply.started":"2021-11-22T07:15:01.390796Z","shell.execute_reply":"2021-11-22T07:15:01.923055Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"scores = pd.DataFrame()\n\nfor e in range(n_epochs): \n    model = MaskRCNN(len(cell_dict))\n    model.load_state_dict(torch.load(f\"model-e{e + 1}.pt\"))\n    model.to(device)\n    scores.loc[e, \"score\"] = get_score(test_dataset, model)\n\nbest_model = \"\"\nscores = scores.sort_values(\"score\", ascending=False)\n\nbest_model = f'model-e{np.argmax(scores[\"score\"]) + 1}.pt'\n\nscores","metadata":{"execution":{"iopub.status.busy":"2021-11-22T06:56:00.286718Z","iopub.execute_input":"2021-11-22T06:56:00.287263Z","iopub.status.idle":"2021-11-22T07:05:42.477081Z","shell.execute_reply.started":"2021-11-22T06:56:00.287224Z","shell.execute_reply":"2021-11-22T07:05:42.476281Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"if best_model == \"\":\n    best_model = \"../input/satorius-models/model-e9.pt\"\n    \nmodel = MaskRCNN(len(cell_dict))\nmodel.load_state_dict(torch.load(best_model))\nmodel.to(device)\nprint()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T07:30:41.338056Z","iopub.execute_input":"2021-11-22T07:30:41.338720Z","iopub.status.idle":"2021-11-22T07:30:42.250634Z","shell.execute_reply.started":"2021-11-22T07:30:41.338680Z","shell.execute_reply":"2021-11-22T07:30:42.249839Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"code","source":"# to-do: include the iou score for the prediction's image\n\nmodel.eval()\ntest_dataset_temp = SatoriusDataset(test_df, \"../input/sartorius-cell-instance-segmentation/train\")\n    \nnum_rows = 3\nfig, ax = plt.subplots(num_rows * batch_size, 3, figsize=(15, 20))\n\nfor i, data in enumerate(test_dataloader): \n    x, y = data\n    x = list(img.to(device) for img in x)\n    \n    with torch.no_grad():\n            predictions = model(x, None)\n    \n    for j in range(batch_size):\n        original_img = test_dataset_temp.__getitem__(i * 2 + j)[0]\n        thresh_hold = mask_threshold_dict[np.array(np.unique(predictions[j][\"labels\"].cpu().numpy(), return_counts=True)).T[0][0]]\n        \n        ax[i * 2 + j, 0].imshow(original_img)\n        ax[i * 2 + j, 1].imshow(combine_masks(y[j][\"masks\"], 0))\n        ax[i * 2 + j, 2].imshow(combine_masks(predictions[j][\"masks\"].squeeze().cpu().numpy(), 0.5))\n        \n        ax[i * 2 + j, 0].axis('off')\n        ax[i * 2 + j, 1].axis('off')\n        ax[i * 2 + j, 2].axis('off')\n\n        ax[i * 2 + j, 1].title.set_text(f'num_cells: {len(y[j][\"masks\"])}')\n        ax[i * 2 + j, 2].title.set_text(f'num_cells: {len(predictions[j][\"masks\"].squeeze().cpu().numpy())}')\n        \n    if i == num_rows - 1: \n        break","metadata":{"execution":{"iopub.status.busy":"2021-11-22T07:37:01.931568Z","iopub.execute_input":"2021-11-22T07:37:01.932307Z","iopub.status.idle":"2021-11-22T07:37:10.172508Z","shell.execute_reply.started":"2021-11-22T07:37:01.932265Z","shell.execute_reply":"2021-11-22T07:37:10.171555Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"### 7. Submission","metadata":{}},{"cell_type":"code","source":"class SatoriusSubmissionDataset(Dataset): \n    def __init__(self, image_dir, transform=None, resize=None):\n        self.transform = transform\n        self.resize = True if resize else False \n        self.image_dir = image_dir\n        self.image_id = [path[:-4] for path in os.listdir(image_dir)]\n        \n        if self.resize: \n            self.height = int(height * resize)\n            self.width = int(width * resize)\n        \n    def __len__(self): \n        return len(self.image_id)\n\n    def __getitem__(self, idx): \n        image_id = self.image_id[idx]\n        image = cv2.imread(os.path.join(self.image_dir, image_id + \".png\"), cv2.IMREAD_COLOR)  \n\n        if self.resize: \n            image = cv2.resize(image, (self.width, self.height))\n        \n        if self.transform: \n            image, _ = self.transform(image, target=None)\n\n        return image, image_id","metadata":{"execution":{"iopub.status.busy":"2021-11-22T07:08:49.041827Z","iopub.execute_input":"2021-11-22T07:08:49.042104Z","iopub.status.idle":"2021-11-22T07:08:49.051551Z","shell.execute_reply.started":"2021-11-22T07:08:49.042069Z","shell.execute_reply":"2021-11-22T07:08:49.050743Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"submission_dataset = SatoriusSubmissionDataset(\"../input/sartorius-cell-instance-segmentation/test\", transform=get_transform(train=False))\nsubmission_dataloader = DataLoader(submission_dataset, batch_size=1, pin_memory=True,\n                             num_workers=2, collate_fn=lambda x: tuple(zip(*x)))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T07:23:30.886646Z","iopub.execute_input":"2021-11-22T07:23:30.887383Z","iopub.status.idle":"2021-11-22T07:23:30.895203Z","shell.execute_reply.started":"2021-11-22T07:23:30.887342Z","shell.execute_reply":"2021-11-22T07:23:30.894515Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"submission = []\n\nfor _, data in enumerate(submission_dataloader): \n    x, y = data\n    x = list(img.to(device) for img in x)\n    \n    with torch.no_grad():\n            predictions = model(x)[0]\n    \n    previous_masks = []\n    for i, mask in enumerate(predictions[\"masks\"]):\n        score = predictions[\"scores\"][i].cpu().item()\n        label = predictions[\"labels\"][i].cpu().item()\n        \n        if score > min_score_dict[label]:\n            mask = mask.cpu().numpy()\n\n            mask = mask > mask_threshold_dict[label]\n            mask = remove_overlapping_pixels(mask, previous_masks)\n            previous_masks.append(mask)\n            rle = rle_encoding(mask)\n            submission.append((y, rle))\n            \n    all_ids = [image_id for image_id, rle in submission]\n    if y not in all_ids:\n        submission.append((image_id, \"\"))\n    \ndf_submission = pd.DataFrame(submission, columns=['id', 'predicted'])\ndf_submission.to_csv(\"submission.csv\", index=False)\ndf_submission.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T07:27:46.001351Z","iopub.execute_input":"2021-11-22T07:27:46.001899Z","iopub.status.idle":"2021-11-22T07:27:48.672705Z","shell.execute_reply.started":"2021-11-22T07:27:46.001855Z","shell.execute_reply":"2021-11-22T07:27:48.671955Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}